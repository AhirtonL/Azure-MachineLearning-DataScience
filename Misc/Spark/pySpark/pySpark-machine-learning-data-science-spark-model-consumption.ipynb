{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring wtih Saved ML Models Generated from the Sampled NYC Taxi Trip and Fare Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we show some how to load models that are stored in blobs, and score data-sets with these stored models.\n",
    "\n",
    "OBJECTIVE: To use models and files to be scroed, that are stored in blob storage, to produce scored data and save that data to blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settting Directory Paths in Mapped Blob Storage Prior to Running\n",
    "\n",
    "Where models/files are being saved in the blob, the path needs to be specified properly. Default container which is attached to the Spark cluster can be referenced as: \"wasb//\".\n",
    "\n",
    "Models are saved in: \"wasb:///user/remoteuser/NYCTaxi/Models\". If this path is not set properly, models will not be loaded for scoring.\n",
    "\n",
    "We save scored results in: \"wasb:///user/remoteuser/NYCTaxi/ScoredResults\". If the path to folder is incorrect, rsutls will not be saved in that folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directory paths to models (input), data to be scored (input), and scored result files (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n",
      "Creating HiveContext as 'sqlContext'\n"
     ]
    }
   ],
   "source": [
    "# 1. LOCATION OF DATA TO BE SCORED (TEST DATA)\n",
    "taxi_test_file_loc = \"wasb://mllibwalkthroughs@cdspsparksamples.blob.core.windows.net/Data/NYCTaxi/JoinedTaxiTripFare.Point1Pct.Test.tsv\";\n",
    "\n",
    "# 2. PATH TO BLOB STORAGE WHICH HAS STORED MODELS WITH WHICH TEST DATA IS TO BE SCORED\n",
    "modelDir = \"wasb:///user/remoteuser/NYCTaxi/Models/\"; # The last backslash is needed;\n",
    "\n",
    "# 3. PATH TO BLOB STORAGE WHERE SCORED RESUTLS WILL BE OUTPUT \n",
    "scoredResultDir = \"wasb:///user/remoteuser/NYCTaxi/ScoredResults/\"; # The last backslash is needed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to specific models to be used for scoring (copy and paste from the bottom of the model training notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logisticRegFileLoc = modelDir + \"LogisticRegressionWithLBFGS_2016-04-2523_40_16.087332\"\n",
    "linearRegFileLoc = modelDir + \"LinearRegressionWithSGD_2016-04-2523_43_30.058765\"\n",
    "randomForestClassificationFileLoc = modelDir + \"RandomForestClassification_2016-04-2523_42_39.585378\"\n",
    "randomForestRegFileLoc = modelDir + \"RandomForestRegression_2016-04-2523_43_52.456436\"\n",
    "BoostedTreeClassificationFileLoc = modelDir + \"GradientBoostingTreeClassification_2016-04-2523_42_55.680761\"\n",
    "BoostedTreeRegressionFileLoc = modelDir + \"GradientBoostingTreeRegression_2016-04-2523_44_10.079694\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRINT START TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2016, 4, 25, 23, 56, 19, 229403)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set spark context and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "import atexit\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion: Read in joined 0.1% taxi trip and fare file (as tsv), format and clean data, and create data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execute above cell: 41.89 seconds"
     ]
    }
   ],
   "source": [
    "timestart = datetime.datetime.now()\n",
    "\n",
    "## IMPORT FILE FROM PUBLIC BLOB\n",
    "\n",
    "taxi_test_file = sc.textFile(taxi_test_file_loc)\n",
    "\n",
    "## GET SCHEMA OF THE FILE FROM HEADER\n",
    "taxi_header = taxi_test_file.filter(lambda l: \"medallion\" in l)\n",
    "\n",
    "## PARSE FIELDS AND CONVERT DATA TYPE FOR SOME FIELDS\n",
    "taxi_temp = taxi_test_file.subtract(taxi_header).map(lambda k: k.split(\"\\t\"))\\\n",
    "        .map(lambda p: (p[0],p[1],p[2],p[3],p[4],p[5],p[6],int(p[7]),int(p[8]),int(p[9]),int(p[10]),\n",
    "                        float(p[11]),float(p[12]),p[13],p[14],p[15],p[16],p[17],p[18],float(p[19]),\n",
    "                        float(p[20]),float(p[21]),float(p[22]),float(p[23]),float(p[24]),int(p[25]),int(p[26])))\n",
    "    \n",
    "## GET SCHEMA OF THE FILE FROM HEADER\n",
    "schema_string = taxi_test_file.first()\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schema_string.split('\\t')]\n",
    "fields[7].dataType = IntegerType() #Pickup hour\n",
    "fields[8].dataType = IntegerType() # Pickup week\n",
    "fields[9].dataType = IntegerType() # Weekday\n",
    "fields[10].dataType = IntegerType() # Passenger count\n",
    "fields[11].dataType = FloatType() # Trip time in secs\n",
    "fields[12].dataType = FloatType() # Trip distance\n",
    "fields[19].dataType = FloatType() # Fare amount\n",
    "fields[20].dataType = FloatType() # Surcharge\n",
    "fields[21].dataType = FloatType() # Mta_tax\n",
    "fields[22].dataType = FloatType() # Tip amount\n",
    "fields[23].dataType = FloatType() # Tolls amount\n",
    "fields[24].dataType = FloatType() # Total amount\n",
    "fields[25].dataType = IntegerType() # Tipped or not\n",
    "fields[26].dataType = IntegerType() # Tip class\n",
    "taxi_schema = StructType(fields)\n",
    "\n",
    "## CREATE DATA FRAME\n",
    "taxi_df_test = sqlContext.createDataFrame(taxi_temp, taxi_schema)\n",
    "\n",
    "## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\n",
    "taxi_df_test_cleaned = taxi_df_test.drop('medallion').drop('hack_license').drop('store_and_fwd_flag').drop('pickup_datetime')\\\n",
    "    .drop('dropoff_datetime').drop('pickup_longitude').drop('pickup_latitude').drop('dropoff_latitude')\\\n",
    "    .drop('dropoff_longitude').drop('tip_class').drop('total_amount').drop('tolls_amount').drop('mta_tax')\\\n",
    "    .drop('direct_distance').drop('surcharge')\\\n",
    "    .filter(\"passenger_count > 0 and passenger_count < 8 AND payment_type in ('CSH', 'CRD') AND tip_amount >= 0 AND tip_amount < 30 AND fare_amount >= 1 AND fare_amount < 150 AND trip_distance > 0 AND trip_distance < 100 AND trip_time_in_secs > 30 AND trip_time_in_secs < 7200\" )\n",
    "\n",
    "## CACHE DATA-FRAME IN MEMORY & MATERIALIZE DF IN MEMORY\n",
    "taxi_df_test_cleaned.cache()\n",
    "taxi_df_test_cleaned.count()\n",
    "\n",
    "## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\n",
    "taxi_df_test_cleaned.registerTempTable(\"taxi_test\")\n",
    "\n",
    "## PRINT HOW MUCH TIME IT TOOK TO RUN THE CELL\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print \"Time taken to execute above cell: \" + str(timedelta) + \" seconds\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation and data prep for scoring with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create traffic time feature, and indexing and one-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execute above cell: 5.67 seconds"
     ]
    }
   ],
   "source": [
    "timestart = datetime.datetime.now()\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, VectorIndexer\n",
    "\n",
    "### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\n",
    "sqlStatement = \"\"\"\n",
    "    SELECT *,\n",
    "    CASE\n",
    "     WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN \"Night\" \n",
    "     WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN \"AMRush\" \n",
    "     WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN \"Afternoon\"\n",
    "     WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN \"PMRush\"\n",
    "    END as TrafficTimeBins\n",
    "    FROM taxi_test \n",
    "\"\"\"\n",
    "taxi_df_test_with_newFeatures = sqlContext.sql(sqlStatement)\n",
    "\n",
    "## CACHE DATA-FRAME IN MEMORY & MATERIALIZE DF IN MEMORY\n",
    "taxi_df_test_with_newFeatures.cache()\n",
    "taxi_df_test_with_newFeatures.count()\n",
    "\n",
    "## INDEX AND ONE-HOT ENCODING\n",
    "stringIndexer = StringIndexer(inputCol=\"vendor_id\", outputCol=\"vendorIndex\")\n",
    "model = stringIndexer.fit(taxi_df_test_with_newFeatures) # Input data-frame is the cleaned one from above\n",
    "indexed = model.transform(taxi_df_test_with_newFeatures)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"vendorIndex\", outputCol=\"vendorVec\")\n",
    "encoded1 = encoder.transform(indexed)\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"rate_code\", outputCol=\"rateIndex\")\n",
    "model = stringIndexer.fit(encoded1)\n",
    "indexed = model.transform(encoded1)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"rateIndex\", outputCol=\"rateVec\")\n",
    "encoded2 = encoder.transform(indexed)\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"payment_type\", outputCol=\"paymentIndex\")\n",
    "model = stringIndexer.fit(encoded2)\n",
    "indexed = model.transform(encoded2)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"paymentIndex\", outputCol=\"paymentVec\")\n",
    "encoded3 = encoder.transform(indexed)\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"TrafficTimeBins\", outputCol=\"TrafficTimeBinsIndex\")\n",
    "model = stringIndexer.fit(encoded3)\n",
    "indexed = model.transform(encoded3)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"TrafficTimeBinsIndex\", outputCol=\"TrafficTimeBinsVec\")\n",
    "encodedFinal = encoder.transform(indexed)\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print \"Time taken to execute above cell: \" + str(timedelta) + \" seconds\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDD objects with feature arrays for input into models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execute above cell: 11.19 seconds"
     ]
    }
   ],
   "source": [
    "timestart = datetime.datetime.now()\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from numpy import array\n",
    "\n",
    "# INDEXING CATEGORICAL TEXT FEATURES FOR INPUT INTO TREE-BASED MODELS\n",
    "def parseRowIndexingBinary(line):\n",
    "    features = np.array([line.paymentIndex, line.vendorIndex, line.rateIndex, line.TrafficTimeBinsIndex,\n",
    "                         line.pickup_hour, line.weekday, line.passenger_count, line.trip_time_in_secs, \n",
    "                         line.trip_distance, line.fare_amount])\n",
    "    return  features\n",
    "\n",
    "# ONE-HOT ENCODING OF CATEGORICAL TEXT FEATURES FOR INPUT INTO LOGISTIC RERESSION MODELS\n",
    "def parseRowOneHotBinary(line):\n",
    "    features = np.concatenate((np.array([line.pickup_hour, line.weekday, line.passenger_count,\n",
    "                                        line.trip_time_in_secs, line.trip_distance, line.fare_amount]), \n",
    "                                        line.vendorVec.toArray(), line.rateVec.toArray(), \n",
    "                                        line.paymentVec.toArray(), line.TrafficTimeBinsVec.toArray()), axis=0)\n",
    "    return  features\n",
    "\n",
    "# ONE-HOT ENCODING OF CATEGORICAL TEXT FEATURES FOR INPUT INTO TREE-BASED MODELS\n",
    "def parseRowIndexingRegression(line):\n",
    "    features = np.array([line.paymentIndex, line.vendorIndex, line.rateIndex, line.TrafficTimeBinsIndex, \n",
    "                         line.pickup_hour, line.weekday, line.passenger_count, line.trip_time_in_secs, \n",
    "                         line.trip_distance, line.fare_amount])\n",
    "    return  features\n",
    "\n",
    "# INDEXING CATEGORICAL TEXT FEATURES FOR INPUT INTO LINEAR REGRESSION MODELS\n",
    "def parseRowOneHotRegression(line):\n",
    "    features = np.concatenate((np.array([line.pickup_hour, line.weekday, line.passenger_count,\n",
    "                                        line.trip_time_in_secs, line.trip_distance, line.fare_amount]), \n",
    "                                        line.vendorVec.toArray(), line.rateVec.toArray(), \n",
    "                                        line.paymentVec.toArray(), line.TrafficTimeBinsVec.toArray()), axis=0)\n",
    "    return  features\n",
    "\n",
    "\n",
    "# FOR BINARY CLASSIFICATION TRAINING AND TESTING\n",
    "indexedTESTbinary = encodedFinal.map(parseRowIndexingBinary)\n",
    "oneHotTESTbinary = encodedFinal.map(parseRowOneHotBinary)\n",
    "\n",
    "# FOR REGRESSION CLASSIFICATION TRAINING AND TESTING\n",
    "indexedTESTreg = encodedFinal.map(parseRowIndexingRegression)\n",
    "oneHotTESTreg = encodedFinal.map(parseRowOneHotRegression)\n",
    "\n",
    "# SCALING FEATURES FOR LINEARREGRESSIONWITHSGD MODEL\n",
    "scaler = StandardScaler(withMean=False, withStd=True).fit(oneHotTESTreg)\n",
    "oneHotTESTregScaled = scaler.transform(oneHotTESTreg)\n",
    "\n",
    "# CACHE RDDS IN MEMORY\n",
    "indexedTESTbinary.cache();\n",
    "oneHotTESTbinary.cache();\n",
    "indexedTESTreg.cache();\n",
    "oneHotTESTreg.cache();\n",
    "oneHotTESTregScaled.cache();\n",
    "\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print \"Time taken to execute above cell: \" + str(timedelta) + \" seconds\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring with saved Logistic Regression Model, and saving output to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execute above cell: 18.24 seconds"
     ]
    }
   ],
   "source": [
    "timestart = datetime.datetime.now()\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "\n",
    "## LOAD SAVED MODEL\n",
    "savedModel = LogisticRegressionModel.load(sc, logisticRegFileLoc)\n",
    "predictions = oneHotTESTbinary.map(lambda features: (float(savedModel.predict(features))))\n",
    "\n",
    "## SAVE SCORED RESULTS (RDD) TO BLOB\n",
    "datestamp = unicode(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "logisticregressionfilename = \"LogisticRegressionWithLBFGS_\" + datestamp + \".txt\";\n",
    "dirfilename = scoredResultDir + logisticregressionfilename;\n",
    "predictions.saveAsTextFile(dirfilename)\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print \"Time taken to execute above cell: \" + str(timedelta) + \" seconds\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Linear Regression Models, and saving output to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execute above cell: 14.37 seconds"
     ]
    }
   ],
   "source": [
    "timestart = datetime.datetime.now()\n",
    "\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LinearRegressionModel\n",
    "\n",
    "# LOAD MODEL AND SCORE USING ** SCALED VARIABLES **\n",
    "savedModel = LinearRegressionModel.load(sc, linearRegFileLoc)\n",
    "predictions = oneHotTESTregScaled.map(lambda features: (float(savedModel.predict(features))))\n",
    "\n",
    "# SAVE RESULTS\n",
    "datestamp = unicode(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "linearregressionfilename = \"LinearRegressionWithSGD_\" + datestamp;\n",
    "dirfilename = scoredResultDir + linearregressionfilename;\n",
    "predictions.saveAsTextFile(dirfilename)\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print \"Time taken to execute above cell: \" + str(timedelta) + \" seconds\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Random Forest Models (Classification and Regression), and saving output to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execute above cell: 27.73 seconds"
     ]
    }
   ],
   "source": [
    "timestart = datetime.datetime.now()\n",
    "\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "## CLASSIFICATION: LOAD SAVED MODEL, SCORE AND SAVE RESULTS BACK TO BLOB\n",
    "savedModel = RandomForestModel.load(sc, randomForestClassificationFileLoc)\n",
    "predictions = savedModel.predict(indexedTESTbinary)\n",
    "\n",
    "# SAVE RESULTS\n",
    "datestamp = unicode(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "rfclassificationfilename = \"RandomForestClassification_\" + datestamp + \".txt\";\n",
    "dirfilename = scoredResultDir + rfclassificationfilename;\n",
    "predictions.saveAsTextFile(dirfilename)\n",
    "\n",
    "####################################################################\n",
    "## REGRESSION: LOAD SAVED MODEL, SCORE AND SAVE RESULTS BACK TO BLOB\n",
    "savedModel = RandomForestModel.load(sc, randomForestRegFileLoc)\n",
    "predictions = savedModel.predict(indexedTESTreg)\n",
    "\n",
    "# SAVE RESULTS\n",
    "datestamp = unicode(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "rfregressionfilename = \"RandomForestRegression_\" + datestamp + \".txt\";\n",
    "dirfilename = scoredResultDir + rfregressionfilename;\n",
    "predictions.saveAsTextFile(dirfilename)\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print \"Time taken to execute above cell: \" + str(timedelta) + \" seconds\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring with saved Gradient Boosting Tree Models (Classification and Regression), and saving output to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execute above cell: 8.14 seconds"
     ]
    }
   ],
   "source": [
    "timestart = datetime.datetime.now()\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "\n",
    "## CLASSIFICATION: LOAD SAVED MODEL, SCORE AND SAVE RESULTS BACK TO BLOB\n",
    "savedModel = GradientBoostedTreesModel.load(sc, BoostedTreeClassificationFileLoc)\n",
    "predictions = savedModel.predict(indexedTESTbinary)\n",
    "\n",
    "# SAVE RESULTS\n",
    "datestamp = unicode(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "btclassificationfilename = \"GradientBoostingTreeClassification_\" + datestamp + \".txt\";\n",
    "dirfilename = scoredResultDir + btclassificationfilename;\n",
    "predictions.saveAsTextFile(dirfilename)\n",
    "\n",
    "####################################################################\n",
    "## REGRESSION: LOAD SAVED MODEL, SCORE AND SAVE RESULTS BACK TO BLOB\n",
    "savedModel = GradientBoostedTreesModel.load(sc, BoostedTreeRegressionFileLoc)\n",
    "predictions = savedModel.predict(indexedTESTreg)\n",
    "\n",
    "# SAVE RESULTS\n",
    "datestamp = unicode(datetime.datetime.now()).replace(' ','').replace(':','_');\n",
    "btregressionfilename = \"GradientBoostingTreeRegression_\" + datestamp + \".txt\";\n",
    "dirfilename = scoredResultDir + btregressionfilename;\n",
    "predictions.saveAsTextFile(dirfilename)\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print \"Time taken to execute above cell: \" + str(timedelta) + \" seconds\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup objects from memory, print final time, and print scored output file locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpersist objects cached in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[70] at mapPartitions at PythonMLLibAPI.scala:1522"
     ]
    }
   ],
   "source": [
    "taxi_df_test_cleaned.unpersist()\n",
    "indexedTESTbinary.unpersist();\n",
    "oneHotTESTbinary.unpersist();\n",
    "indexedTESTreg.unpersist();\n",
    "oneHotTESTreg.unpersist();\n",
    "oneHotTESTregScaled.unpersist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out path to scored output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logisticRegFileLoc: LogisticRegressionWithLBFGS_2016-04-2523_57_25.716141.txt\n",
      "linearRegFileLoc: LinearRegressionWithSGD_2016-04-2523_57_44.240388\n",
      "randomForestClassificationFileLoc: RandomForestClassification_2016-04-2523_57_59.161879.txt\n",
      "randomForestRegFileLoc: RandomForestRegression_2016-04-2523_58_13.226642.txt\n",
      "BoostedTreeClassificationFileLoc: GradientBoostingTreeClassification_2016-04-2523_58_28.273022.txt\n",
      "BoostedTreeRegressionFileLoc: GradientBoostingTreeRegression_2016-04-2523_58_32.570847.txt"
     ]
    }
   ],
   "source": [
    "print \"logisticRegFileLoc: \" + logisticregressionfilename;\n",
    "print \"linearRegFileLoc: \" + linearregressionfilename;\n",
    "print \"randomForestClassificationFileLoc: \" + rfclassificationfilename;\n",
    "print \"randomForestRegFileLoc: \" + rfregressionfilename;\n",
    "print \"BoostedTreeClassificationFileLoc: \" + btclassificationfilename;\n",
    "print \"BoostedTreeRegressionFileLoc: \" + btregressionfilename;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print finish time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2016, 4, 25, 23, 58, 35, 858545)"
     ]
    }
   ],
   "source": [
    "datetime.datetime.now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}