---
title: "Using SparkR and sparklyr with 2013 NYCTaxi Data: Data manipulations, modeling, and evaluation"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: "Algorithms and Data Science, Microsoft Data Group"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: espresso
    keep_md: yes
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
runtime: knit
---

<hr>
#Introduction
This Markdown document shows the use of <a href="http://spark.rstudio.com/index.html" target="_blank">sparklyr</a> package for featurization and creating ML models in spark context. If you are interested in performing data manipulations using <a href="https://spark.apache.org/docs/1.6.0/sparkr.html" target="_blank">SparkR (1.6)</a>, you can look at the more comprehensive markdown published <a href="https://spark.apache.org/docs/1.6.0/sparkr.html" target="_blank">here (see SparkR_sparklyr_NYCTaxi or SparkR_sparklyr_NYCTaxi.Rmd)</a>.

<br>

The data used for this exercise is the public NYC Taxi Trip and Fare data-set (2013, December, ~4 Gb, ~13 million rows) available from: http://www.andresmh.com/nyctaxitrips. Data for this exercise can be downloaded from the public blob (see below). The data can be uploaded to the blob (or other storage) attached to your HDInsight cluster (HDFS) and used as input into the scripts shown here. We have used 10% of the data from the month of December 2013 for the examples show here. Where necessary, small amounts of data is brought to the local data frames for plotting and visualization. 

<hr>
<br>


#Creating spark context / connections and loading required packages
```{r Load Packages, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# CREATE SPARK CONTEXT
###########################################
list.files(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)

sparkEnvir <- list(spark.executor.instance = '10', spark.yarn.executor.memoryOverhead = '8000')
sc <- sparkR.init(
  sparkEnvir = sparkEnvir,
  sparkPackages = "com.databricks:spark-csv_2.10:1.3.0"
)
sqlContext <- sparkRSQL.init(sc)

###########################################
# LOAD LIBRARIES FROM SPECIFIED PATH
###########################################
library(rmarkdown)
library(knitr)
library(gridExtra)
library(sparklyr)
library(dplyr)
library(DBI)
library(sparkapi)
library(ggplot2)
###########################################
## CREATE SPARKLYR SPARK CONNECTION
###########################################
sp <- spark_connect(master = "yarn-client")
sqlContextSPR <- sparkRSQL.init(sp)
```
<hr>

#Reading in data & visualize
The 2013 NYC taxi trip and fare data-set consists of 12 pairs of files, one for fare and trip for each of the 12 months (http://www.andresmh.com/nyctaxitrips). Raw data for this exercise can be downloaded from the public blob locations below: 
<br>
1. Trip (Csv): http://cdspsparksamples.blob.core.windows.net/data/NYCTaxi/KDD2016/trip_data_12.csv
<br>
2. Fare (Csv): http://cdspsparksamples.blob.core.windows.net/data/NYCTaxi/KDD2016/trip_fare_12.csv
<br>

The data can be uploaded to the blob (or other storage) attached to your HDInsight cluster (HDFS) and used as input into the scripts. In the The csv files can be read into Spark context and saved in parquet format. Once saved in parquet format, data can be read in much more quickly than csv files.

<br>

The trip and fare files need to be joined on multiple keys to prepare one joined file, which can then be used as input for modeling. We have shown how to join and manipulate data using SparkR in another <a href="https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/KDDCup2016/Code/SparkR" target="_blank">script file (see SparklyR_NYCTaxi.html or SparklyR_NYCTaxi.Rmd)</a>.

<br>

We have sampled 10% of the December 2013 data (~1.4 mil rows) as input into sparklyr functions. This data is saved in parquet format for fast reading, and contains features such as trip distance, trip time, pickup hour, etc. for modeling the amount of tip paid for each taxi trip.

<br> 

##Load joined trip-fare data in sparklyr spark connection and cache
If a data-set is large, it may need to be down-sampled for modeling in reasonable amount of time. Here we used the <b>sample</b> function from SparkR to down-sample the joined tax-fare data. We then save the data in HDFS for use as input into the sparklyr modeling functions.
```{r Load data in sparklyr dataframe, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# LOAD SAMPLED JOINED TAXI DATA FROM HDFS, CACHE
###########################################
joinedDF <- spark_read_parquet(sp, name = "joined_table", 
                               path = "/HdiSamples/HdiSamples/NYCTaxi/JoinedParquetSampledFile", memory = TRUE, overwrite = TRUE)
head(joinedDF, 5)

###########################################
# SHOW THE NUMBER OF OBSERVATIONS IN DATA 
###########################################
count(joinedDF)
```

<br>

##Visualize data
For visualization, a small portion data will have to be sampled and brought into local memory as a data.frame object. R’s plotting functions (e.g. from those in ggplot package) can then be applied to the data.frame for visualization.
```{r Data visualization, message=FALSE, warning=FALSE, echo=TRUE, fig.width=8, fig.height=4}
###########################################
# SAMPLE SMALL PORTION (1000 rows) OF DATA 
###########################################
joinedSampled <- dplyr::sample_n(joinedDF, 1000)

###########################################
# CONVERT SPARK DF TO LOCAL DATA.FRAME IN MEMORY OF R-SERVER EDGE NODE
###########################################
joinedSampledDF <- as.data.frame(joinedSampled);

###########################################
# PLOT HISTOGRAM OF TIP AMOUNT
###########################################
hist <- ggplot(joinedSampledDF, aes(x=tip_amount)) + 
  geom_histogram(binwidth = 0.5, aes(fill = ..count..)) + 
  scale_fill_gradient("Count", low = "green", high = "red") + 
  labs(title="Histogram for Tip Amount");

###########################################
# PLOT HISTOGRAM OF TIP AMOUNT
###########################################
scatter <- ggplot(joinedSampledDF, aes(tip_amount, trip_distance)) + 
  geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + 
  labs(title="Tip amount vs. trip distance");

grid.arrange(hist, scatter, ncol=2)
```

<hr>
<hr>

#Modeling problem - predicting amount of tip for taxi trips (Regression)

The modeling objective is to predict the amount ($) of tips paid for taxi trips based on features such as trip distance, trip time, pickup hour, payment type (cast, credit), etc. Predictions are evaluated against the actual tip amount using the R-squred metric.


<hr>
<br> 

#sparklyr for creating ML models
sparklyr provides bindings to Spark’s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package. Together with sparklyr’s dplyr interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.

<br>

##Use feature transformation functions from sparklyr
Spark provides feature transformers, faciliating many common transformations of data within in a Spark DataFrame, and sparklyr exposes these within the <a href="http://spark.rstudio.com/mllib.html#transformers" target="_blank">ft_* family of functions</a>. These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns. Here, as examples, we show the use of two such functions to bucketize (categorize) or binarize features. Payment type (CSH or CRD) is binarized using string-indexer and binerizer functions. And, traffic-time bins is bucketized using the bucketizer function.
```{r Using ft_ functions for feature transformation, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# CREATE TRANSFORMED FEATURES, BINAZURE OR BUCKET FEATURES
###########################################
# Binarizer
joinedDF2 <- joinedDF %>% ft_string_indexer(input_col = 'payment_type', output_col = 'pt_ind') %>% ft_binarizer(input_col = 'pt_ind', output_col = 'pt_bin', threshold = 0.5)

head(joinedDF2, 5)
# Bucketizer
joinedDF3 <- joinedDF2 %>% ft_string_indexer(input_col = 'TrafficTimeBins', output_col = 'TrafficTimeInd') %>% ft_bucketizer(input_col = 'TrafficTimeInd', output_col = 'TrafficTimeBuc', splits=c(-1,0.5,1.5,2.5,3.5))

head(joinedDF3, 5)
```

<hr>
##Create train-test partitions
Data can be partitioned into training and testing using the <b>sdf_partition</b> function. Training data partition is used to create models. Predictions and evaluation of predictive accuracy is performed on the test data partition.
```{r Partition data into train/test, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# CREATE TRAIN/TEST PARTITIONS
###########################################
partitions <- joinedDF3 %>% sdf_partition(training = 0.75, test = 0.25, seed = 1099)
```
<hr>

##Creating ML models
Spark’s machine learning library can be accessed from sparklyr through the <a href="http://spark.rstudio.com/mllib.html#algorithms" target="_blank">ml_* family of functions</a>. Here we create ML models for the prediction of tip-amount for taxi trips.

###Creating Elastic Net model
Create a elastic net model using training data, and evaluate on test data-set
```{r Elastic net modeo, message=FALSE, warning=FALSE, echo=TRUE, fig.width=5, fig.height=4}
# Fit elastic net regression model
fit <- partitions$training %>% ml_linear_regression(response = "tip_amount", features = c("pt_bin", "fare_amount", "pickup_hour", "passenger_count", "trip_distance", "TrafficTimeBuc"), alpha = 0.5, lambda = 0.01)

# Show summary of fitted Elastic Net model
summary(fit)

# Predict on test data & evaluate in local data-frame
predictedVals <- predict(fit, newdata =  partitions$test)
predictedVals2 <- partitions$test %>% select(tip_amount) %>% collect %>% mutate(fitted = predictedVals)
predictedDF <- as.data.frame(predictedVals2)

# Predict on test data and keep predictions in Spark context
predictedVals <- sdf_predict(fit, newdata =  partitions$test)

# Evaluate and plot predictions (R-sqr)
Rsqr = cor(predictedDF$tip_amount, predictedDF$fitted)^2; Rsqr;

# Sample predictions for plotting
predictedDFSampled <- predictedDF[base::sample(1:nrow(predictedDF), 1000),]

# Plot actual vs. predicted tip amounts
lm_model <- lm(fitted ~ tip_amount, data = predictedDFSampled)
ggplot(predictedDFSampled, aes(tip_amount, fitted)) + geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + geom_abline(aes(slope = summary(lm_model)$coefficients[2,1], intercept = summary(lm_model)$coefficients[1,1]), color = "red")
```

###Creating Random Forest Model
Create a random forest model using training data, and evaluate on test data-set
```{r Random forest model, message=FALSE, warning=FALSE, echo=TRUE, fig.width=5, fig.height=4}
# Fit Random Forest regression model
fit <- partitions$training %>% ml_random_forest(response = "tip_amount", features = c("pt_bin", "fare_amount", "pickup_hour", "passenger_count",  "trip_distance", "TrafficTimeBuc"), max.bins = 500L, max.depth = 5L, num.trees = 50L)

# Show summary of fitted Random Forest model
summary(fit)

# Predict on test data & evaluate in local data-frame
predictedVals <- predict(fit, newdata =  partitions$test)
predictedVals2 <- partitions$test %>% select(tip_amount) %>% collect %>% mutate(fitted = predictedVals)
predictedDF <- as.data.frame(predictedVals2)

# Predict on test data and keep predictions in Spark context
predictedVals <- sdf_predict(fit, newdata =  partitions$test)

# Evaluate and plot predictions (R-sqr)
Rsqr = cor(predictedDF$tip_amount, predictedDF$fitted)^2; Rsqr;

# Sample predictions for plotting
predictedDFSampled <- predictedDF[base::sample(1:nrow(predictedDF), 1000),]

# Plot
lm_model <- lm(fitted ~ tip_amount, data = predictedDFSampled)
ggplot(predictedDFSampled, aes(tip_amount, fitted)) + geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + geom_abline(aes(slope = summary(lm_model)$coefficients[2,1], intercept = summary(lm_model)$coefficients[1,1]), color = "red")
```


###Creating Gradient Boosted Tree Model
Create a gradient boosted tree model using training data, and evaluate on test data-set
```{r Boosted tree model, message=FALSE, warning=FALSE, echo=TRUE, fig.width=5, fig.height=4}
# Fit Gradient Boosted Tree regression model
fit <- partitions$training %>% ml_gradient_boosted_trees(response = "tip_amount", features = c("pt_bin", "fare_amount","pickup_hour","passenger_count","trip_distance","TrafficTimeBuc"), max.bins = 32L, max.depth = 3L, type = "regression")

# Show summary of fitted Random Forest model
summary(fit)

# Predict on test data & evaluate in local data-frame
predictedVals <- predict(fit, newdata =  partitions$test)
predictedVals2 <- partitions$test %>% select(tip_amount) %>% collect %>% mutate(fitted = predictedVals)
predictedDF <- as.data.frame(predictedVals2)

# Predict on test data and keep predictions in Spark context
predictedVals <- sdf_predict(fit, newdata =  partitions$test)

# Evaluate and plot predictions (R-sqr)
Rsqr = cor(predictedDF$tip_amount, predictedDF$fitted)^2; Rsqr;

# Sample predictions for plotting
predictedDFSampled <- predictedDF[base::sample(1:nrow(predictedDF), 1000),]

# Plot
lm_model <- lm(fitted ~ tip_amount, data = predictedDFSampled)
ggplot(predictedDFSampled, aes(tip_amount, fitted)) + geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + geom_abline(aes(slope = summary(lm_model)$coefficients[2,1], intercept = summary(lm_model)$coefficients[1,1]), color = "red")
```

<br>
<hr>
<hr>
<br>

#Summary
The examples shown here can be adopted to fit other data exploration and modeling scenarios having different data-types or prediction tasks (e.g. classification)
