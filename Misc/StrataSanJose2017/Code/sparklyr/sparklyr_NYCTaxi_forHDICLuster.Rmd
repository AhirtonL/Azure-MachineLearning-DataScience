---
title: "Using sparklyr with 2013 NYCTaxi Data: Featurization, modeling, and evaluation"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: "Algorithms and Data Science & R Server Teams, Microsoft Data Group"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: haddock
    keep_md: yes
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
runtime: knit
---

<hr>
#Introduction
This Markdown document shows the use of <a href="http://spark.rstudio.com/index.html" target="_blank">sparklyr</a> for feature engineering and creating machine learning models. The data used for this exercise is the public NYC Taxi Trip and Fare data-set (2013, sampled to about ~13 million rows) available from: http://www.andresmh.com/nyctaxitrips. Data for this exercise can be downloaded from the public blob (see below). The data can be uploaded to the blob (or other storage) attached to your HDInsight cluster (HDFS) and used as input into the scripts shown here.

sparklyr provides bindings to Spark’s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package. Together with sparklyr’s dplyr interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.

Where necessary, small amounts of data is brought to the local data frames for plotting and visualization. 
<hr>
<br>

<br>

#Creating spark context / connections and loading required packages
```{r Load Packages, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# LOAD LIBRARIES FROM SPECIFIED PATH
###########################################
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
library(rmarkdown)
library(knitr)
library(sparklyr)
library(dplyr)
library(DBI)
library(gridExtra)
library(ggplot2)

###########################################
## CREATE SPARKLYR SPARK CONNECTION
###########################################
sp <- spark_connect(master = "yarn-client")

###########################################
## SPECIFY BASE HDFS DIRECTORY
###########################################
fullDataDir <- "/HdiSamples/HdiSamples/NYCTaxi"
```
<hr>
<br>
<br>

#Load trip-fare data and cache sample in memory
The taxi and fare files were previously joined using SparkR SQL and down-sampled to about 10% of the full dataset.
```{r Load data in sparklyr dataframe, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# LOAD SAMPLED JOINED TAXI DATA FROM HDFS, CACHE
###########################################
starttime <- Sys.time()

joinedFilePath <- file.path(fullDataDir, "NYCjoinedParquetSubset")
joinedDF <- spark_read_parquet(sp, name = "joined_table", 
                               path = joinedFilePath, memory = TRUE, 
                               overwrite = TRUE)
tbl_cache(sp, "joined_table", force=TRUE)
head(joinedDF, 3)

###########################################
# SAMPLE AND CACHE IN MEMORY
###########################################
joinedDFSampl <- sdf_sample(joinedDF, fraction = 1)

###########################################
# SHOW THE NUMBER OF OBSERVATIONS IN DATA 
###########################################
count(joinedDFSampl)

endtime <- Sys.time()
print (endtime - starttime)
```

<hr>
#Transformations using sparklyr functions
Spark provides feature transformers, faciliating many common transformations of data within in a Spark DataFrame, and sparklyr exposes these within the <a href="http://spark.rstudio.com/mllib.html#transformers" target="_blank">ft_* family of functions</a>. These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns. Here, we show the use of two such functions to bucketize (categorize) or binarize features. Payment type (CSH or CRD) is binarized using string-indexer and binerizer functions. And, traffic-time bins is bucketized using the bucketizer function.
```{r Using ft_ functions for feature transformation, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# CREATE TRANSFORMED FEATURES, BINARIZE PAYMENT-TYPE
###########################################
starttime <- Sys.time()

# Binarizer
joinedDF2 <- joinedDFSampl %>% ft_string_indexer(input_col = 'payment_type', 
                                            output_col = 'payment_ind') %>% 
                                            ft_binarizer(input_col = 'payment_ind', 
                                                        output_col = 'pay_type_bin', 
                                                        threshold = 0.5)
head(joinedDF2)

endtime <- Sys.time()
print (endtime - starttime)
```

<hr>
#Create train-test partitions
Data can be partitioned into training and testing using the <b>sdf_partition</b> function. 
```{r Partition data into train/test, message=FALSE, warning=FALSE, echo=TRUE}
starttime <- Sys.time()

###########################################
# CREATE TRAIN/TEST PARTITIONS
###########################################
partitions <- joinedDF2 %>% sdf_partition(training = 0.70, 
                                          test = 0.30, seed = 123)

endtime <- Sys.time()
print (endtime - starttime)
```
<hr>

#Using sparklyr for creating ML models
Spark’s machine learning library can be accessed from sparklyr through the <a href="http://spark.rstudio.com/mllib.html#algorithms" target="_blank">ml_* family of functions</a>. Here we create ML models for the prediction of tip-amount for taxi trips.

##Creating Elastic Net model
Create a elastic net model using training data, and evaluate on test data-set
```{r Elastic net modeo, message=FALSE, warning=FALSE, echo=TRUE, fig.width=5, fig.height=4}
starttime <- Sys.time()

###########################################
# FIT ELASTIC NET REGRESSION MODEL
###########################################
fit <- partitions$training %>% 
      ml_linear_regression(tip_amount ~ 
                          pay_type_bin + pickup_hour + passenger_count + 
                          trip_distance + TrafficTimeBins, 
                          alpha = 0.5, lambda = 0.01)

###########################################
# SHOW MODEL SUMMARY
###########################################
summary(fit)

###########################################
# PREDICT ON TEST DATA, AND EVALUATE
###########################################
predictedVals <- sdf_predict(fit, newdata =  partitions$test)
predictedValsSampled <- sdf_sample(x=predictedVals, 
                                   fraction=0.0001, replacement=FALSE, seed=123)
predictedDF <- as.data.frame(predictedValsSampled)

Rsqr = cor(predictedDF$tip_amount, predictedDF$prediction)^2; Rsqr;

# Sample predictions for plotting
predictedDFSampled <- predictedDF[base::sample(1:nrow(predictedDF), 1000),]

# Plot actual vs. predicted tip amounts
lm_model <- lm(prediction ~ tip_amount, data = predictedDFSampled)
ggplot(predictedDFSampled, aes(tip_amount, prediction)) + 
  geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + 
  geom_abline(aes(slope = summary(lm_model)$coefficients[2,1], 
                  intercept = summary(lm_model)$coefficients[1,1]), 
              color = "red")

###########################################
# SAVE PREDICTIONS TO A CSV FILE IN HDFS
###########################################
sparklyRPredictionsPath <- file.path(fullDataDir, "sparklyRElasticNetPredictions")
spark_write_csv(predictedVals, sparklyRPredictionsPath)

endtime <- Sys.time()
print (endtime - starttime)
```

##Creating Random Forest Model
Create a random forest model using training data, and evaluate on test data-set
```{r Random forest model, message=FALSE, warning=FALSE, echo=TRUE, fig.width=10, fig.height=5}
starttime <- Sys.time()

###########################################
# FIT RANDOM FOREST REGRESSION MODEL
###########################################
fit <- ml_random_forest(x=partitions$training, 
                        response = "tip_amount", 
                        features = c("pay_type_bin", "fare_amount", 
                                     "pickup_hour", "passenger_count",  
                                     "trip_distance", "TrafficTimeBins"), 
                        max.bins = 32L, max.depth = 4L, num.trees = 10L)

###########################################
# SHOW SUMMARY OF RANDOM FOREST MODEL
###########################################
summary(fit)

###########################################
# PLOT FEATURE IMPORTANCE
###########################################
feature_importance <- ml_tree_feature_importance(sp, fit) %>%
    mutate(importance = as.numeric(levels(importance))[importance]) %>%
    mutate(feature = as.character(feature));

plot1 <- feature_importance %>%
  ggplot(aes(reorder(feature, importance), importance)) + 
  geom_bar(stat = "identity", fill = 'darkgreen') + coord_flip() + xlab("") +
  ggtitle("Feature Importance")


###########################################
# PREDICT ON TEST SET AND EVALUATE
###########################################
predictedVals <- sdf_predict(fit, newdata =  partitions$test)
predictedValsSampled <- sdf_sample(x=predictedVals, 
                                   fraction=0.0001, replacement=FALSE, seed=123)
predictedDF <- as.data.frame(predictedValsSampled)

# Evaluate and plot predictions (R-sqr)
Rsqr = cor(predictedDF$tip_amount, predictedDF$prediction)^2; Rsqr;

# Sample predictions for plotting
predictedDFSampled <- predictedDF[base::sample(1:nrow(predictedDF), 1000),]

# Plot
lm_model <- lm(prediction ~ tip_amount, data = predictedDFSampled)
plot2 <- ggplot(predictedDFSampled, aes(tip_amount, prediction)) + 
  geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + 
  geom_abline(aes(slope = summary(lm_model)$coefficients[2,1], 
                  intercept = summary(lm_model)$coefficients[1,1]), color = "red")

grid.arrange(plot1, plot2, ncol=2)

endtime <- Sys.time()
print (endtime - starttime)
```


##Creating Gradient Boosted Tree Model
Create a gradient boosted tree model using training data, and evaluate on test data-set
```{r Boosted tree model, message=FALSE, warning=FALSE, echo=TRUE, fig.width=10, fig.height=5}
starttime <- Sys.time()

###########################################
# FIT GRADIENT BOOSTED TREE REGRESSION MODEL
###########################################
fit <- partitions$training %>% 
       ml_gradient_boosted_trees(tip_amount ~ 
                                pay_type_bin + pickup_hour + passenger_count + 
                                trip_distance + TrafficTimeBins, 
                                max.bins = 32L, max.depth = 4L, type = "regression")

###########################################
# SHOW SUMMARY OF MODEL
###########################################
summary(fit)

###########################################
# PLOT FEATURE IMPORTANCE OF GBT MODEL
###########################################
feature_importance <- ml_tree_feature_importance(sp, fit) %>%
    mutate(importance = as.numeric(levels(importance))[importance]) %>%
    mutate(feature = as.character(feature));

plot1 <- feature_importance %>%
  ggplot(aes(reorder(feature, importance), importance)) + 
  geom_bar(stat = "identity", fill = 'darkgreen') + coord_flip() + xlab("") +
  ggtitle("Feature Importance")

###########################################
# PREDICT ON TEST SET AND EVALUATE
###########################################
predictedVals <- sdf_predict(fit, newdata =  partitions$test)
predictedValsSampled <- sdf_sample(x=predictedVals, 
                                   fraction=0.0001, replacement=FALSE, seed=123)
predictedDF <- as.data.frame(predictedValsSampled)

# Evaluate and plot predictions (R-sqr)
Rsqr = cor(predictedDF$tip_amount, predictedDF$prediction)^2; Rsqr;

# Sample predictions for plotting
predictedDFSampled <- predictedDF[base::sample(1:nrow(predictedDF), 1000),]

# Plot
lm_model <- lm(prediction ~ tip_amount, data = predictedDFSampled)
plot2 <- ggplot(predictedDFSampled, aes(tip_amount, prediction)) + 
  geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + 
  geom_abline(aes(slope = summary(lm_model)$coefficients[2,1], 
                  intercept = summary(lm_model)$coefficients[1,1]), 
              color = "red")

grid.arrange(plot1, plot2, ncol=2)

endtime <- Sys.time()
print (endtime - starttime)
```

#Uncache objects, and disconnect Spark session
```{r Uncache objects and disconnect Spark, message=FALSE, warning=FALSE, echo=TRUE, fig.width=10, fig.height=5}
###########################################
# UNCACHE TABLES
###########################################
tbl_uncache(sp, "joined_table")

###########################################
# DISCONNECT SPARK CONNECTION
###########################################
spark_disconnect(sp)
```

<br>
<hr>
<hr>
<br>

#Summary
The examples shown here can be adopted to fit other data exploration and modeling scenarios having different data-types or prediction tasks (e.g. classification)