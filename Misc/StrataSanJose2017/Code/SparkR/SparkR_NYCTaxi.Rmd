---
title: "Using SparkR and sparklyr with 2013 NYCTaxi Data: Data manipulations, modeling, and evaluation"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: "Algorithms and Data Science, Microsoft Data Group"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: espresso
    keep_md: yes
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
runtime: knit
---

<hr>
#Introduction
This Markdown document shows the use of <a href="https://spark.apache.org/docs/latest/sparkr.html" target="_blank">SparkR</a> and <a href="http://spark.rstudio.com/index.html" target="_blank">sparklyr</a> packages for data manipulation, and creating machine learning models in spark context. The data used for this exercise is the public NYC Taxi Trip and Fare data-set (2013, December, ~4 Gb, ~13 million rows) available from: http://www.andresmh.com/nyctaxitrips. Data for this exercise can be downloaded from the public blob (see below). The data can be uploaded to the blob (or other storage) attached to your HDInsight cluster (HDFS) and used as input into the scripts shown here.

We use SparkR for data manipulations (e.g. data joining) and sparklyr for creating and evaluating models. Where necessary, small amounts of data is brought to the local data frames for plotting and visualization.
<hr>
<br>

#Using SparkR for data wrangling & manipulation
SparkR is an R package that provides a light-weight frontend to use Apache Spark from R. In Spark 1.6, SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc. (similar to R data frames, dplyr) but on large datasets. SparkR also provides limited support for distributed machine learning using MLlib.

<br>

##Creating spark context / connections and loading required packages
```{r Load Packages, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# LOAD LIBRARIES FROM SPECIFIED PATH
###########################################
Sys.setenv(YARN_CONF_DIR="/opt/hadoop/current/etc/hadoop", HADOOP_HOME="/opt/hadoop/current", 
           JAVA_HOME = "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64",
           SPARK_HOME = "/dsvm/tools/spark/current",
           PATH=paste0(Sys.getenv("PATH"),":/opt/hadoop/current/bin"))

list.files(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
library(rmarkdown)
library(knitr)
library(gridExtra)
library(sparklyr)
library(dplyr)
library(DBI)
library(ggplot2)

###########################################
# CREATE SPARK CONTEXT
###########################################
sparkEnvir <- list(spark.executor.instance = '10', spark.yarn.executor.memoryOverhead = '8000')
sc <- sparkR.session(
  sparkEnvir = sparkEnvir,
  sparkPackages = "com.databricks:spark-csv_2.10:1.3.0"
)

###########################################
## SPECIFY BASE HDFS DIRECTORY
###########################################
fullDataDir <- "/user/RevoShare/remoteuser/Data"
```
<hr>

##Reading in files from HDFS (csv or parquet format) and manipulate using SQL
Data for this exercise can be downloaded from the public blob locations below: 
<br>
1. Trip (Csv): http://cdspsparksamples.blob.core.windows.net/data/NYCTaxi/KDD2016/trip_data_12.csv
<br>
2. Fare (Csv): http://cdspsparksamples.blob.core.windows.net/data/NYCTaxi/KDD2016/trip_fare_12.csv
<br>
The data can be uploaded to the blob (or other storage) attached to your HDInsight cluster (HDFS) and used as input into the scripts shown here. The csv files can be read into Spark context and saved in parquet format. Once saved in parquet format, data can be read in much more quickly than csv files.

###You can read in raw files in csv.
```{r Read in files, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# TRIP FILE (CSV format)
###########################################
tripPath <- file.path(fullDataDir, "trip_data_12.csv")
tripDF <- read.df(tripPath, source = "com.databricks.spark.csv", 
                  header = "true", inferSchema = "true")
head(tripDF, 3)
printSchema(tripDF)

###########################################
# FARE FILE (parquet format)
###########################################
farePath <- file.path(fullDataDir, "trip_fare_12.csv")
fareDF <- read.df(farePath, source = "com.databricks.spark.csv", 
                  header = "true", inferSchema = "true")
head(fareDF, 3)
printSchema(fareDF)
```

###Register tables and join using SQL. 
You can register dataframes as tables in SQLContext and join using multiple columns. The following SQL also filters the data for some outliers.
```{r Register tables, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# 1. REGISTER TABLES AND JOIN ON MULTIPLE COLUMNS, FILTER DATA
# 2. REGISTER JIONED TABLE
###########################################
createOrReplaceTempView(tripDF, "trip")
createOrReplaceTempView(fareDF, "fare")

trip_fareDF <-  SparkR::sql("SELECT 
  hour(f.pickup_datetime) as pickup_hour, f.vendor_id, f.fare_amount, 
  f.surcharge, f.tolls_amount, f.tip_amount, f.payment_type, t.rate_code, 
  t.passenger_count, t.trip_distance, t.trip_time_in_secs 
  FROM trip t, fare f  
  WHERE t.medallion = f.medallion AND t.hack_license = f.hack_license 
  AND t.pickup_datetime = f.pickup_datetime 
  AND t.passenger_count > 0 and t.passenger_count < 8 
  AND f.tip_amount >= 0 AND f.tip_amount <= 15 
  AND f.fare_amount >= 1 AND f.fare_amount <= 150 
  AND f.tip_amount < f.fare_amount AND t.trip_distance > 0 
  AND t.trip_distance <= 40 AND t.trip_time_in_secs >= 30 
  AND t.trip_time_in_secs <= 7200 AND t.rate_code <= 5
  AND f.payment_type in ('CSH','CRD')")
createOrReplaceTempView(trip_fareDF, "trip_fare")

###########################################
# SHOW REGISTERED TABLES
###########################################
head(SparkR::sql("show tables"))
```


###Feature engineering using SQL 
You can create new features using sQL statements. For example, you can use case statements to generate categorical features from coneunuous (numerical) ones.
```{r Feature engineering, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# CREATE FEATURES IN SQL USING CASE STATEMENTS
###########################################
trip_fare_feat <- SparkR::sql("SELECT 
    payment_type, pickup_hour, fare_amount, tip_amount, 
    passenger_count, trip_distance, trip_time_in_secs, 
  CASE
    WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN 'Night'
    WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN 'AMRush' 
    WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN 'Afternoon'
    WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN 'PMRush'
    END as TrafficTimeBins,
  CASE
    WHEN (tip_amount > 0) THEN 1 
    WHEN (tip_amount <= 0) THEN 0 
    END as tipped
  FROM trip_fare")

SparkR::persist(trip_fare_feat, "MEMORY_ONLY")
SparkR::count(trip_fare_feat)
head(trip_fare_feat, 3)
```
<hr>

##Data visualization
For visualization, a small portion data will have to be sampled and brought into local memory as a data.frame object. R's plotting functions (e.g. from those in ggplot package) can then be applied to the data.frame for visualization.
```{r Exploration and visualization, message=FALSE, warning=FALSE, echo=TRUE, fig.width=8, fig.height=4}
###########################################
# SAMPLE SMALL PORTION OF DATA
###########################################
trip_fare_featSampled <- SparkR::sample(trip_fare_feat, withReplacement=FALSE, 
                                fraction=0.0001, seed=123)

###########################################
# CONVERT SPARK DF TO LOCAL DATA.FRAME IN MEMORY OF R-SERVER EDGE NODE
###########################################
trip_fare_featSampledDF <- as.data.frame(trip_fare_featSampled);

###########################################
# Generate HISTOGRAM OF TIP AMOUNT
###########################################
hist <- ggplot(trip_fare_featSampledDF, aes(x=tip_amount)) + 
  geom_histogram(binwidth = 0.5, aes(fill = ..count..)) + 
  scale_fill_gradient("Count", low = "green", high = "red") + 
  labs(title="Histogram for Tip Amount");

###########################################
# Generate Scatter Plot OF TRIP DISTANCE vs. TIP AMOUNT
###########################################
scatter <- ggplot(trip_fare_featSampledDF, aes(tip_amount, trip_distance)) + 
  geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + 
  labs(title="Tip amount vs. trip distance");

###########################################
# Plot Histogram and Scatter Plot OF TIP AMOUNT Side by Side
###########################################
grid.arrange(hist, scatter, ncol=2)
```


##Down-sample data for modeling
If a data-set is large, it may need to be down-sampled for modeling in reasonable amount of time. Here we used the <b>sample</b> function from SparkR to down-sample the joined trip-fare data. We then save the data in HDFS for use as input into the sparklyr modeling functions.
```{r Downsample and save data, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# SAMPLE DATA FOR MODELING
###########################################
trip_fare_featSampled <- sample(trip_fare_feat, withReplacement=FALSE, 
                                fraction=0.1, seed=123)

###########################################
# REPARTITION DATA FOR SAVING
###########################################
trip_fare_featSampledRepartitioned <- repartition(trip_fare_featSampled, 10) # write.df below will produce this many files

###########################################
# SAVE DATAFRANE AS PARQUET file
###########################################
write.df(df=trip_fare_featSampledRepartitioned, 
         path=file.path(fullDataDir, "NYCjoinedParquetSubset"), source="parquet", mode="overwrite")

###########################################
# UNPERSIST CACHED DATA FRAME
###########################################
SparkR::unpersist(trip_fare_feat)

###########################################
# STOP SPARKR CONTEXT
###########################################
sparkR.stop()
```

<br>
<hr>
<hr>
<br>

#Summary
The examples shown here can be adopted to fit other data exploration and wrangling scenarios using SparkR.