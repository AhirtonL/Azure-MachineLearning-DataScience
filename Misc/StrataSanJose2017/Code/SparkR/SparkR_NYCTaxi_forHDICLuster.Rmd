---
title: "Using SparkR with 2013 NYCTaxi Data: Data wrangling, manipulations, modeling, and evaluation"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: "Algorithms and Data Science & R Server Teams, Microsoft Data Group"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: haddock
    keep_md: yes
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
runtime: knit
---

<hr>
#Introduction
This Markdown document shows the use of <a href="https://spark.apache.org/docs/latest/sparkr.html" target="_blank">SparkR</a> for data wrangling, manipulation, and creating machine learning models. The data used for this exercise is the public NYC Taxi Trip and Fare data-set (2013,  ~45 Gb, ~140 million rows) available from: http://www.andresmh.com/nyctaxitrips. Data for this exercise can be downloaded from the public blob (see below). The data can be uploaded to the blob (or other storage) attached to your HDInsight cluster (HDFS) and used as input into the scripts shown here.

We use Spark SQL for many of the data wrangling tasks. For plotting and visualization, small amounts of data from Spark dataframes are transformed to the local data frames.
<hr>
<br>

#Creating spark context and loading packages
```{r Load Packages, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# LOAD LIBRARIES FROM SPECIFIED PATH
###########################################
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
library(rmarkdown)
library(knitr)
library(gridExtra)
library(ggplot2)
library(ggmap)

###########################################
# CREATE SPARK CONTEXT
###########################################
sc <- sparkR.session(
  sparkPackages = "com.databricks:spark-csv_2.10:1.3.0"
)
SparkR::setLogLevel("OFF")

###########################################
## SPECIFY BASE HDFS DIRECTORY
###########################################
fullDataDir <- "/HdiSamples/HdiSamples/NYCTaxi"
```
<hr>


#Reading in files from HDFS (csv or parquet)
Data for this exercise can be downloaded from the public blob locations below: 
<br>
1. Trip (Csv): http://cdspsparksamples.blob.core.windows.net/data/NYCTaxi/combined_taxi_trip_w_header.csv (~19 Gb)
<br>
2. Fare (Csv): http://cdspsparksamples.blob.core.windows.net/data/NYCTaxi/combined_taxi_fare_w_header.csv (~28 Gb)
<br>
The data can be uploaded to the blob (or other storage) attached to your HDInsight cluster (HDFS) and used as input into the scripts shown here. The csv files can be read into Spark context and saved in parquet format. Once saved in parquet format, data can be read in much more quickly than csv files.
```{r Read in files, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# TRIP FILE (CSV format, or PARQUET format)
###########################################
starttime <- Sys.time();

#tripPath <- file.path(fullDataDir, "combined_taxi_trip_w_header.csv")
#tripDF <- read.df(tripPath, source = "com.databricks.spark.csv", 
#                  header = "true", inferSchema = "true")
#tripPathParquet <- file.path(fullDataDir, "CombinedTaxiTripParquet2013")
#tripDFRepartitioned <- repartition(tripDF, 20) # write.df below will produce this many files
#write.parquet(tripDFRepartitioned, tripPathParquet)

tripPathParquet <- file.path(fullDataDir, "CombinedTaxiTripParquet2013")
tripDF <- read.parquet(tripPathParquet)
head(tripDF, 3)
printSchema(tripDF)

###########################################
# FARE FILE (CSV or PARQUET format)
###########################################
#farePath <- file.path(fullDataDir, "combined_taxi_fare_w_header.csv")
#fareDF <- read.df(farePath, source = "com.databricks.spark.csv", 
#                  header = "true", inferSchema = "true")
#farePathParquet <- file.path(fullDataDir, "CombinedTaxiFareParquet2013")
#fareDFRepartitioned <- repartition(fareDF, 20) # write.df below will produce this many files
#write.parquet(fareDFRepartitioned, farePathParquet)

farePathParquet <- file.path(fullDataDir, "CombinedTaxiFareParquet2013")
fareDF <- read.parquet(farePathParquet)
SparkR::cache(fareDF); SparkR::count(fareDF);
head(fareDF, 3)
printSchema(fareDF)

endtime <- Sys.time();
print (endtime-starttime);
```


#Using SparkR for data wrangling & manipulation
SparkR is an R package that provides a light-weight frontend to use Apache Spark from R. In Spark 2.0, SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc. (similar to R data frames, dplyr) but on large datasets. SparkR also provides support for distributed machine learning using MLlib.
<br>

##Join datasets using SQL 
You can register dataframes as tables in SQLContext and join using multiple columns. The following SQL also filters the data for some outliers.
```{r Register tables, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# 1. REGISTER TABLES AND JOIN ON MULTIPLE COLUMNS, FILTER DATA
# 2. REGISTER JIONED TABLE
###########################################
starttime <- Sys.time();

createOrReplaceTempView(tripDF, "trip")
createOrReplaceTempView(fareDF, "fare")

trip_fareDF <-  SparkR::sql("SELECT 
  f.pickup_datetime, hour(f.pickup_datetime) as pickup_hour, 
  t.dropoff_datetime, hour(t.dropoff_datetime) as dropoff_hour,
  f.vendor_id, f.fare_amount, f.surcharge, f.tolls_amount, 
  f.tip_amount, f.payment_type, t.rate_code, 
  t.passenger_count, t.trip_distance, t.trip_time_in_secs, 
  t.pickup_longitude, t.pickup_latitude, t.dropoff_longitude, 
  t.dropoff_latitude
  FROM trip t, fare f  
  WHERE t.medallion = f.medallion AND t.hack_license = f.hack_license 
  AND t.pickup_datetime = f.pickup_datetime 
  AND t.passenger_count > 0 and t.passenger_count < 8 
  AND f.tip_amount >= 0 AND f.tip_amount <= 15 
  AND f.fare_amount >= 1 AND f.fare_amount <= 150 
  AND f.tip_amount < f.fare_amount AND t.trip_distance > 0 
  AND t.trip_distance <= 40 AND t.trip_distance >= 1
  AND t.trip_time_in_secs >= 30 AND t.trip_time_in_secs <= 7200 
  AND t.rate_code <= 5 AND f.payment_type in ('CSH','CRD')")
createOrReplaceTempView(trip_fareDF, "trip_fare")
SparkR::cache(trip_fareDF); SparkR::count(trip_fareDF);

###########################################
# WRITE JOINED FILE IN PARQUET FORMAT IN HDFS
###########################################
#tripfarePathParquet <- file.path(fullDataDir, "CombinedTaxi_Trip_and_Fare_Parquet2013")
#tripfareDFRepartitioned <- repartition(trip_fareDF, 20) # write.df below will produce this many files
#write.parquet(tripfareDFRepartitioned, tripfarePathParquet)

###########################################
# SHOW REGISTERED TABLES
###########################################
head(SparkR::sql("show tables"))

###########################################
# IF THE JOINED FILE IS SAVED, YOU CAN DIRECTLY READ IT IN
###########################################
#tripfarePathParquet <- file.path(fullDataDir, "CombinedTaxi_Trip_and_Fare_Parquet2013")
#trip_fareDF <- read.parquet(tripfarePathParquet)
#head(trip_fareDF, 3)
#printSchema(trip_fareDF)
#createOrReplaceTempView(trip_fareDF, "trip_fare")

endtime <- Sys.time();
print (endtime-starttime);
```


#Feature engineering using SQL 
You can create new features using sQL statements. For example, you can use case statements to generate categorical features from coneunuous (numerical) ones.
```{r Feature engineering, message=FALSE, warning=FALSE, echo=TRUE}
starttime <- Sys.time();

###########################################
# CREATE FEATURES IN SQL USING CASE STATEMENTS
###########################################
trip_fare_feat <- SparkR::sql("SELECT 
    payment_type, pickup_hour, fare_amount, tip_amount, 
    passenger_count, trip_distance, trip_time_in_secs, 
  CASE
    WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN 'Night'
    WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN 'AMRush' 
    WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN 'Afternoon'
    WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN 'PMRush'
    END as TrafficTimeBins,
  CASE
    WHEN (tip_amount > 0) THEN 1 
    WHEN (tip_amount <= 0) THEN 0 
    END as tipped
  FROM trip_fare")

SparkR::cache(trip_fare_feat); SparkR::count(trip_fare_feat);
createOrReplaceTempView(trip_fare_feat, "trip_fare_feat")
head(trip_fare_feat, 3)

endtime <- Sys.time();
print (endtime-starttime);
```
<hr>

#Data visualization
##Data exploration and plotting
For visualization, a small portion data will have to be sampled and brought into local memory as a data.frame object. R's plotting functions (e.g. from those in ggplot package) can then be applied to the data.frame for visualization.
```{r Exploration and visualization, message=FALSE, warning=FALSE, echo=TRUE, fig.width=10, fig.height=4}
starttime <- Sys.time();

###########################################
# SAMPLE SMALL PORTION OF DATA
###########################################
trip_fare_featSampled <- SparkR::sample(trip_fare_feat, withReplacement=FALSE, 
                                fraction=0.00001, seed=123)

###########################################
# CONVERT SPARK DF TO LOCAL DATA.FRAME IN MEMORY OF R-SERVER EDGE NODE
###########################################
trip_fare_featSampledDF <- as.data.frame(trip_fare_featSampled);

###########################################
# Generate HISTOGRAM OF TIP AMOUNT
###########################################
hist <- ggplot(trip_fare_featSampledDF, aes(x=tip_amount)) + 
  geom_histogram(binwidth = 0.5, aes(fill = ..count..)) + 
  scale_fill_gradient("Count", low = "green", high = "red") + 
  labs(title="Histogram for Tip Amount");

###########################################
# Generate Scatter Plot OF TRIP DISTANCE vs. TIP AMOUNT
###########################################
scatter <- ggplot(trip_fare_featSampledDF, aes(tip_amount, trip_distance)) + 
  geom_point(col='darkgreen', alpha=0.3, pch=19, cex=2) + 
  labs(title="Tip amount vs. trip distance");

###########################################
# Plot Histogram and Scatter Plot OF TIP AMOUNT Side by Side
###########################################
grid.arrange(hist, scatter, ncol=2)

endtime <- Sys.time();
print (endtime-starttime);
```

##Advanced SQL summarization and plotting:
###Trips in NYC area during rush and non-rush hours
This section shows more examples of SQL and advanced plotting and visualization, using ggmap. We plot the number of trips by day of the month, as well as number of trips on the NY City map during rush and non-rush hours.
```{r Advanced SQL and Visualization 1, message=FALSE, warning=FALSE, echo=TRUE, fig.width=10, fig.height=4}
starttime <- Sys.time();

###########################################
# GROUP TRIPS BY YEAR, MONTH, DAY, PAYMENT TYPE
###########################################
trip_stats_by_day <- SparkR::sql("select 
      year(pickup_datetime) as year, month(pickup_datetime) as month, 
      day(pickup_datetime) as day, payment_type, count(1) as trips 
      from fare 
      where payment_type in ('CSH','CRD')
      group by year(pickup_datetime), month(pickup_datetime), 
      day(pickup_datetime), payment_type")
tsbd <- as.data.frame(trip_stats_by_day)

###########################################
# PLOT NUMBER OF TRIPS BY DAY IN DEC 2013
###########################################
ggplot(data=tsbd, aes(day, trips)) + geom_point(aes(color=payment_type)) + geom_smooth(aes(color=payment_type))

###########################################
# PLOT TAXI PICKUPS BY HOUR TRAFFIC AND NON-TRAFFIC
###########################################
library(ggmap);
# Now, for the mapping of the rides; first we get the map
nyc_geocode <- geocode("New York City")
nyc_map13 <- get_map(location=c(nyc_geocode$lon, nyc_geocode$lat), zoom=13)

trips_2014_June_18_5am <- SparkR::sql("select * from trip_fare where hour(pickup_datetime) = 5 and day(pickup_datetime) = 11 and month(pickup_datetime) = 12 and year(pickup_datetime) = 2013 and trip_distance > 2")
trips_2014_June_18_5amDF <- as.data.frame(trips_2014_June_18_5am)
map1 <- ggmap(nyc_map13) + geom_point(data=trips_2014_June_18_5amDF, aes(pickup_longitude, pickup_latitude), color="darkred", alpha=0.04)


trips_2014_June_18_5pm <- SparkR::sql("select * from trip_fare where hour(pickup_datetime) = 17 and day(pickup_datetime) = 11 and month(pickup_datetime) = 12 and year(pickup_datetime) = 2013 and trip_distance > 2")
trips_2014_June_18_5pmDF <- as.data.frame(trips_2014_June_18_5pm)
map2 <- ggmap(nyc_map13) + geom_point(data=trips_2014_June_18_5pmDF, aes(pickup_longitude, pickup_latitude), color="darkred", alpha=0.04)

grid.arrange(map1, map2, ncol=2)

endtime <- Sys.time();
print (endtime-starttime);
```

###Net efflux from NY City during AM and PM rush hours
This section examples of advanced SQL, as well as plotting. Nex efflux is shown on NY City maps during morning and evening rush hours.
```{r Advanced SQL and Visualization 2, message=FALSE, warning=FALSE, echo=TRUE, fig.width=10, fig.height=4}
starttime <- Sys.time();

###########################################
# PLOT MORNING EFFLUX AT 8 AM
###########################################
morning_efflux <- SparkR::sql("select lon, lat, 
      sum(flux) as net_flux from
      (select round(pickup_longitude, 3) as lon, 
      round(pickup_latitude, 3) as lat, count(1) flux
      from trip_fare
      where hour(pickup_datetime) = 8 and 
      day(pickup_datetime) = 11 and 
      month(pickup_datetime) = 12 and 
      year(pickup_datetime) = 2013
      and trip_distance > 2
      group by round(pickup_longitude, 3), round(pickup_latitude, 3)
      union all 
      select round(pickup_longitude, 3) as lon, 
      round(pickup_latitude, 3) as lat, count(1) * -1 flux
      from trip_fare
      where hour(dropoff_datetime) = 8 and 
      day(dropoff_datetime) = 11 and 
      month(dropoff_datetime) = 12 and year(dropoff_datetime) = 2013
      and trip_distance > 2
      group by round(pickup_longitude, 3), round(pickup_latitude, 3)
      ) piecewise_flux
      group by lon, lat")
morning_effluxDF <- as.data.frame(morning_efflux)
map1 <- ggmap(nyc_map13) + geom_point(data=morning_effluxDF, aes(lon, lat, color=net_flux), alpha = 0.4, size=0.5) + scale_color_continuous(low="blue", high="red")


###########################################
# PLOT EVENIG EFFLUX AT 6 PM
###########################################
evening_efflux <- SparkR::sql("select lon, lat, sum(flux) as net_flux from
      (select round(pickup_longitude, 3) as lon, 
      round(pickup_latitude, 3) as lat, count(1) flux
      from trip_fare
      where hour(pickup_datetime) = 18 and 
      day(pickup_datetime) = 11 and 
      month(pickup_datetime) = 12 and 
      year(pickup_datetime) = 2013 
      and trip_distance > 2
      group by round(pickup_longitude, 3), round(pickup_latitude, 3)
      union all 
      select round(pickup_longitude, 3) as lon, 
      round(pickup_latitude, 3) as lat, count(1) * -1 flux
      from trip_fare
      where hour(dropoff_datetime) = 18 and 
      day(dropoff_datetime) = 11 and 
      month(dropoff_datetime) = 12 and year(dropoff_datetime) = 2013
      and trip_distance > 2
      group by round(pickup_longitude, 3), round(pickup_latitude, 3)
      ) piecewise_flux
      group by lon, lat")
evening_efflux <- as.data.frame(evening_efflux)
map2 <- ggmap(nyc_map13) + geom_point(data=evening_efflux, aes(lon, lat, color=net_flux), alpha = 0.4, size=0.5) + scale_color_continuous(low="red", high="darkgreen")

grid.arrange(map1, map2, ncol=2)

endtime <- Sys.time();
print (endtime-starttime);
```


#Modeling with SparkR
##Down-sampling data for modeling
If a data-set is large, it may need to be down-sampled for modeling in reasonable amount of time. Here we used the <b>sample</b> function from SparkR to down-sample the joined trip-fare data. We then save the data in HDFS for use as input into the sparklyr modeling functions.
```{r Downsample data for training, message=FALSE, warning=FALSE, echo=TRUE}
starttime <- Sys.time();

###########################################
# SAMPLE DATA FOR MODELING
###########################################
trip_fare_featSampled <- sample(trip_fare_feat, withReplacement=FALSE, 
                                fraction=1, seed=123)
SparkR::cache(trip_fare_featSampled); SparkR::count(trip_fare_featSampled);
createOrReplaceTempView(trip_fare_featSampled, "trip_fare_featSampled")

endtime <- Sys.time();
print (endtime-starttime);
```

##Partition data into train/test
```{r Partition data, message=FALSE, warning=FALSE, echo=TRUE, fig.width=5, fig.height=4}
starttime <- Sys.time();

###########################################
# PARTITION DATA INTO TRAIN-TEST USIN SQL
###########################################
dfrand <- SparkR::sql("SELECT *, RAND() as randnum from trip_fare_featSampled" );
trainDF <- SparkR::filter(dfrand, dfrand$randnum <= 0.7)
testDF <- SparkR::filter(dfrand, dfrand$randnum > 0.7)
```

##Create a SparkR::glm model
Train a glm model.
```{r Glm train, message=FALSE, warning=FALSE, echo=TRUE, fig.width=5, fig.height=4}
starttime <- Sys.time();

########################################### 
## CREATE GLM MODEL
###########################################
model <- SparkR::spark.glm(tip_amount ~ payment_type + pickup_hour + 
                    fare_amount + passenger_count + trip_distance + 
                    trip_time_in_secs + TrafficTimeBins, 
                    data = trainDF, family = "gaussian", 
                    tol = 1e-05, maxIter = 10)
print (summary(model));

endtime <- Sys.time();
print (endtime-starttime);
```

##Evaluate predictions on test set
```{r Evaluate glm model, message=FALSE, warning=FALSE, echo=TRUE, fig.width=5, fig.height=4}
starttime <- Sys.time();

########################################### 
## PREDICT ON TEST SET, AND EVALUATE ACCURACY
###########################################
predictions <- SparkR::predict(model, newData = testDF)
predfilt <- SparkR::select(predictions, c("label","prediction"))
SparkR::cache(predfilt); SparkR::count(predfilt);

## SAMPLE PREDICTIONS FOR EVALUATION USINGS R DATAFRAME
predfiltSampled <- sample(predfilt, withReplacement=FALSE, 
                                fraction=0.0001, seed=123)
predfilt_local <- as.data.frame(predfiltSampled)

# EVALUATE AND PLOT PREDICTIONS (R-sqr)
Rsqr = cor(predfilt_local$label, predfilt_local$prediction)^2; Rsqr;

# Sample predictions for plotting
predictedSampled <- predfilt_local[base::sample(1:nrow(predfilt_local), 1000),]

# Plot predicted vs. actual values
lm_model <- lm(prediction ~ label, data = predictedSampled)
ggplot(predictedSampled, aes(label, prediction)) + geom_point(col='darkgreen', alpha=0.3, pch=19, cex=1.5)   + geom_abline(aes(slope = summary(lm_model)$coefficients[2,1], intercept = summary(lm_model)$coefficients[1,1]), color = "red")

endtime <- Sys.time();
print (endtime-starttime);
```


#Save data & model
##Save model for deployment
Partition data into train/test, and train a glm model and evaluate it.
```{r Persist model, message=FALSE, warning=FALSE, echo=TRUE}
starttime <- Sys.time();

###########################################
## SAVE MODEL [REMOVE FILE IF IT EXISTS]
###########################################
if (length(system("hadoop fs -ls /HdiSamples/HdiSamples/NYCTaxi/SparkGlmModel", intern=TRUE))>=1) {
  system("hadoop fs -rm -r /HdiSamples/HdiSamples/NYCTaxi/SparkGlmModel")
}
modelPath =  file.path(fullDataDir, "SparkGlmModel");
write.ml(model, modelPath) 
```

##Save predictions to CSV
```{r Save predictions to csv, message=FALSE, warning=FALSE, echo=TRUE}
starttime <- Sys.time();

###########################################
## REPARTITION DATA FOR SAVING, AND WRITE TO CSV
###########################################
predictFile <- file.path(fullDataDir, "SparkRGLMPredictions")
predfiltRepartition <- repartition(predfilt, 10)

write.df(df=predfiltRepartition, path=predictFile, 
        source = "com.databricks.spark.csv", 
        mode = "overwrite")

endtime <- Sys.time();
print (endtime-starttime);
```

##Save joined and sampled data
Repartition the data in specific number of chunks and save
```{r Save data, message=FALSE, warning=FALSE, echo=TRUE}
starttime <- Sys.time();

###########################################
# REPARTITION DATA FOR SAVING, AND WRITE TO PARQUET FILE
###########################################
joinedFilePath <- file.path(fullDataDir, "NYCjoinedParquetSubset")

trip_fare_featSampledRepartitioned <- repartition(trip_fare_feat, 10) # write.df below will produce this many files
write.df(df=trip_fare_featSampledRepartitioned, 
         path=joinedFilePath, 
         source="parquet", mode="overwrite")


endtime <- Sys.time();
print (endtime-starttime);
```

#Clear cache and exit
```{r Clear chace and exit Spark, message=FALSE, warning=FALSE, echo=TRUE}
###########################################
# UNPERSIST CACHED DATA FRAMES
###########################################
SparkR::unpersist(fareDF)
SparkR::unpersist(trip_fareDF)
SparkR::unpersist(trip_fare_feat)
SparkR::unpersist(trip_fare_featSampled)
SparkR::unpersist(predfilt)

###########################################
# STOP SPARKR CONTEXT
###########################################
sparkR.stop()
```

<br>
<hr>
<hr>
<br>

#Summary
The examples shown here can be adopted to fit other data exploration and modeling scenarios having different data-types or prediction tasks (e.g. classification).